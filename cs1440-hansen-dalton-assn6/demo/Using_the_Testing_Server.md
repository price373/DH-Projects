# Using the Testing server

`testing_server.py` is a little HTTP server that makes up new web pages as it
is browsed.  It tracks how many times each page is visited, and displays a
report.  Because infinite loops can easily occur in recursive programs it
restarts itself after a few seconds.

This program lets you work on the assignment when the internet is unavailable
as well as helps you troubleshoot tricky crawler problems.


## Quick start

In one shell run:

```
$ python demo/testing_server.py
Serving from http://localhost:8000/
Press Ctrl-C or visit http://localhost:8000/shutdown/ to quit

```

In another shell run your crawler directed at the URL indicated:

```
$ python src/crawler.py http://localhost:8000/
```

## Command line interface

### Usage:
    python test_server.py [-host localhost] [-port 8000]
    python test_server.py -help


### Specify a different port on the command line

If port `8000` is unavailable on your computer you will get an `OSError`:

```
$ python test_server.py
OSError: [Errno 98] Address already in use
```

This happens when another program on your computer is using that port.  Try
other port numbers until you find one that is available.  Valid port numbers
run from `1024` to `65535`.

```
$ python demo/testing_server.py -port 4321
Serving from http://localhost:4321/
Press Ctrl-C or visit http://localhost:4321/shutdown/ to quit
```


### Example: Listen on http://localhost:8000
    python test_server.py

### Example: Listen on http://localhost:4444
    python test_server.py -port 4444

### Example: Listen on http://0.0.0.0:8888
    python test_server.py -host 0.0.0.0 -port 8888

`-host 0.0.0.0` makes the server accessible from OUTSIDE your computer, provided that you know your computer's IP address and your firewall allows it.  Do this ONLY if you understand what this entails and accept the risk!


## Special pages

Visit `/restart` to restart the server immediately; new options may be given once the server comes back up.

Visit `/shutdown` to stop the server and return to the shell.


## Server Options

Server options are set from directives encoded in the path of the *first* URL visited in a session.  After the server restarts itself, a new selection of options can easily be set.

Directives are specified as name=value, separated by slashes (/). The order that directives are specified does not matter. If the same directive is specified multiple times, the last value seen is used.

*   `/restart=[true|false]` controls whether the server automatically restarts itself after the timer expires (Default is True)
*   `/timer=[0-N]` how long before the server restarts itself (Default is 3 seconds)
    *   Specify /timer=0 to disable the auto-restart timer; this lets you explore the server from the comfort of your web browser
*   `/depth=[0-N]` how deep links generated by the server go before they wrap back around to the beginning (Default is 4 levels)
*   `/breadth=[1-62]` how many new branches to create from each page (Default is 3)
*   `/fragments=[true|false]` whether to create links with URL #fragments (Default is True)
*   `/landmines=[true|false]` when enabled, visiting a link for the second time causes the server to immediately restart (Default is False)
*   `/reset=[0..100]` a percentage of pages containing a "reset" link which triggers a `ConnectionError` in the crawler (Default is 0%)
*   `/deadend=[true|false]` controls whether a "dead-end" link is included on each page (Default is True)
*   `/verbose=[true|false]` when set, makes the server produce a longer report at the end(Default is False)

### Example: Disable the timer and produce a detailed report at the end (the report is shown by visiting http://127.0.0.1:8000/restart)
    $ python src/crawler.py http://127.0.0.1:8000/timer=0/verbose=true/

### Example: Set the timer to 60 seconds, disable land mines, the dead end page, and set the maximum depth to 30
    $ python src/crawler.py http://127.0.0.1:8000/timer=60/landmines=false/deadend=false/depth=30

### Example: Set the breadth to 1 and maximum depth to 20
    $ python src/crawler.py http://127.0.0.1:8000/breadth=1/depth=20

### Example: The same as above, but disable fragments in links
    $ python src/crawler.py http://127.0.0.1:8000/breadth=1/depth=20/fragments=false

### Example: Place a "reset" link on 50% of generated pages to test that the crawler handles `ConnectionError` without crashing
    $ python src/crawler.py http://127.0.0.1:8000/reset=50/


### Make the server run forever to explore it in your browser

The testing server shuts itself down after a few seconds.  This stops your
crawler if it gets stuck in an infinite loop.  When you set the timer to `0`,
it will remain running long enough for you to explore it in your browser.  This
will help you understand what your crawler sees and how it should behave.

Start the server and visit `http://127.0.0.1:8000/timer=0/`.

When you are done, visit `http://127.0.0.1:8000/restart` to reset the server and make it ready for your crawler.


## Understanding the report

After the server shuts down it prints a report of pages visited with the number
of times each was visited.  Your crawler should visit each page once and *only*
once.  Your crawler *cannot* visit every link it sees.  It must respect its
base case and quit at some point.

This is what a visit by a good crawler to `http://localhost:8000/` will look
like (notice the trailing `/` in the URL):

```
41 pages were visited exactly once

81 pages were not visited at all
```

Because URLs that don't end in `/` can mean the same thing as URLs that do,
this is also good output from a crawler to `http://localhost:8000`:

```
4 pages were visited exactly once

9 pages were not visited at all

1 page was visited many times
	/: 2
```

The page `/` is visited twice is because your crawler thinks
`http://localhost:8000` and `http://localhost:8000/` are different pages while
the server thinks they are the same.  Don't worry about this, life is too short
and precious to waste on this inconsistency.  When grading your submission this
silly distinction will not be held against you.

If your crawler just doesn't know when to quit you'll see output like this:

```
81 pages were not visited at all

41 pages were visited many times
	/: 68
	/deadend: 67
	/a: 20
	/b: 20
	/c: 20
	/aa: 8
	/ab: 8
    ...
```

If your crawler is just wildly guessing URLs it will visit pages before they
exist.  If you see `N pages were visited before they were created` you need to
seriously re-think your approach.


## What's a "BrokenPipeError"?

It's a harmless error.  Do not be alarmed if you see this while viewing the server in your browser.  It happens when the server doesn't respond well to a feature of your browser.
